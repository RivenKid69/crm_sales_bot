"""
Full Voice Bot Pipeline: STT -> LLM -> TTS
Real-time voice conversation with timing metrics
"""
import time
import subprocess
import numpy as np
import sounddevice as sd
import soundfile as sf
import ollama
from pathlib import Path
from dataclasses import dataclass

from faster_whisper import WhisperModel


SAMPLE_RATE = 16000
AUDIO_DIR = Path(__file__).parent / "audio"
AUDIO_DIR.mkdir(exist_ok=True)

MODEL_DIR = Path(__file__).parent / "models"
PIPER_MODEL = MODEL_DIR / "ru_RU-irina-medium.onnx"


@dataclass
class PipelineMetrics:
    """Timing metrics for the pipeline"""
    stt_time: float = 0.0
    llm_time: float = 0.0
    llm_first_token: float = 0.0
    tts_time: float = 0.0
    total_time: float = 0.0
    audio_input_duration: float = 0.0
    audio_output_duration: float = 0.0

    def print_summary(self):
        print("\n" + "=" * 60)
        print("ğŸ“Š Pipeline Metrics")
        print("=" * 60)
        print(f"ğŸ¤ STT time:           {self.stt_time:.2f}s")
        print(f"ğŸ¤– LLM first token:    {self.llm_first_token:.2f}s")
        print(f"ğŸ¤– LLM total time:     {self.llm_time:.2f}s")
        print(f"ğŸ”Š TTS time:           {self.tts_time:.2f}s")
        print("-" * 60)
        print(f"â±ï¸  Total pipeline:     {self.total_time:.2f}s")
        print(f"ğŸ¤ Input audio:        {self.audio_input_duration:.2f}s")
        print(f"ğŸ”Š Output audio:       {self.audio_output_duration:.2f}s")
        print(f"âš¡ Latency (to first): {self.stt_time + self.llm_first_token:.2f}s")


class VoicePipeline:
    """Full voice conversation pipeline"""

    def __init__(
        self,
        whisper_model: str = "base",
        llm_model: str = "qwen2.5:7b",
    ):
        self.llm_model = llm_model

        print("=" * 60)
        print("ğŸš€ Initializing Voice Pipeline")
        print("=" * 60)

        # Initialize STT
        print("\nğŸ“¥ Loading Whisper model...")
        stt_start = time.time()
        self.stt = WhisperModel(
            whisper_model,
            device="cpu",
            compute_type="int8"
        )
        print(f"   âœ… Whisper loaded in {time.time() - stt_start:.2f}s")

        # Check Piper model
        if not PIPER_MODEL.exists():
            raise FileNotFoundError(f"Piper model not found: {PIPER_MODEL}")
        print(f"   âœ… Piper model: {PIPER_MODEL.name}")

        # System prompt
        self.system_prompt = """Ğ¢Ñ‹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, ĞºĞ°Ğº Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğµ.
Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ“Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾.
ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 2-3 Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸."""

        print("\nâœ… Pipeline ready!")

    def record_audio(self, duration: float = 5.0) -> np.ndarray:
        """Record audio from microphone"""
        print(f"\nğŸ¤ Recording ({duration}s)... Speak now!")
        audio = sd.rec(
            int(duration * SAMPLE_RATE),
            samplerate=SAMPLE_RATE,
            channels=1,
            dtype=np.float32
        )
        sd.wait()
        print("âœ… Recording complete")
        return audio.flatten()

    def speech_to_text(self, audio: np.ndarray) -> tuple[str, float]:
        """Convert speech to text"""
        temp_path = AUDIO_DIR / "temp_input.wav"
        sf.write(temp_path, audio, SAMPLE_RATE)

        start = time.time()
        segments, _ = self.stt.transcribe(
            str(temp_path),
            language="ru",
            beam_size=5,
            vad_filter=True
        )
        text = " ".join([s.text for s in segments]).strip()
        elapsed = time.time() - start

        return text, elapsed

    def text_to_speech(self, text: str) -> tuple[np.ndarray, float]:
        """Convert text to speech using Piper"""
        output_path = AUDIO_DIR / "temp_output.wav"

        start = time.time()

        cmd = [
            "piper",
            "--model", str(PIPER_MODEL),
            "--output_file", str(output_path)
        ]

        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        process.communicate(input=text.encode("utf-8"))

        elapsed = time.time() - start

        audio, sr = sf.read(output_path)
        return audio, sr, elapsed

    def play_audio(self, audio: np.ndarray, sample_rate: int = 22050):
        """Play audio"""
        sd.play(audio, sample_rate)
        sd.wait()

    def run_conversation(self, record_duration: float = 5.0) -> PipelineMetrics:
        """Run full conversation pipeline"""
        metrics = PipelineMetrics()
        pipeline_start = time.time()

        # Step 1: Record
        audio_input = self.record_audio(record_duration)
        metrics.audio_input_duration = record_duration

        # Step 2: STT
        print("\nğŸ”„ Transcribing...")
        user_text, stt_time = self.speech_to_text(audio_input)
        metrics.stt_time = stt_time
        print(f"ğŸ“ You said: {user_text}")

        if not user_text.strip():
            print("âš ï¸  No speech detected")
            return metrics

        # Step 3: LLM (with streaming output)
        print("\nğŸ¤– Assistant: ", end="", flush=True)
        llm_start = time.time()
        first_token_time = None
        full_response = ""

        stream = ollama.chat(
            model=self.llm_model,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_text}
            ],
            stream=True
        )

        for chunk in stream:
            if first_token_time is None:
                first_token_time = time.time() - llm_start
            content = chunk["message"]["content"]
            full_response += content
            print(content, end="", flush=True)

        print()
        metrics.llm_first_token = first_token_time or 0
        metrics.llm_time = time.time() - llm_start

        # Step 4: TTS
        print("\nğŸ”Š Synthesizing speech...")
        audio_output, sr, tts_time = self.text_to_speech(full_response)
        metrics.tts_time = tts_time
        metrics.audio_output_duration = len(audio_output) / sr

        # Step 5: Play
        print("â–¶ï¸  Playing response...")
        self.play_audio(audio_output, sr)

        metrics.total_time = time.time() - pipeline_start
        return metrics


def main():
    """Interactive voice bot"""
    print("\n" + "=" * 60)
    print("ğŸ™ï¸  Voice Bot Pipeline Test")
    print("=" * 60)

    # Initialize pipeline
    pipeline = VoicePipeline(
        whisper_model="base",
        llm_model="qwen2.5:7b"
    )

    print("\n" + "=" * 60)
    print("ğŸ“¢ Ready for conversation!")
    print("   Press Enter to start recording (5 seconds)")
    print("   Type 'q' to quit")
    print("=" * 60)

    while True:
        user_input = input("\nâ Press Enter to speak (or 'q' to quit): ")
        if user_input.lower() == 'q':
            print("ğŸ‘‹ Goodbye!")
            break

        metrics = pipeline.run_conversation(record_duration=5.0)
        metrics.print_summary()


if __name__ == "__main__":
    main()
