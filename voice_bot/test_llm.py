"""
Test LLM with Ollama
Tests response generation and streaming
"""
import time
import ollama
from rich.console import Console
from rich.live import Live
from rich.text import Text

console = Console()


def test_llm_basic(model: str = "qwen2.5:7b"):
    """Test basic LLM response"""
    print("=" * 50)
    print(f"ü§ñ LLM Test (Ollama - {model})")
    print("=" * 50)

    prompt = "–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –∫—Ä–∞—Ç–∫–æ, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?"

    print(f"\nüìù Prompt: {prompt}")
    print("\nüîÑ Generating response...")

    start_time = time.time()

    response = ollama.chat(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
    )

    elapsed = time.time() - start_time
    text = response["message"]["content"]

    print("\n" + "=" * 50)
    print("üìä Results:")
    print("=" * 50)
    print(f"üí¨ Response: {text}")
    print(f"\n‚è±Ô∏è  Total time: {elapsed:.2f}s")

    return text, elapsed


def test_llm_streaming(model: str = "qwen2.5:7b"):
    """Test streaming LLM response"""
    print("\n" + "=" * 50)
    print(f"üåä LLM Streaming Test (Ollama - {model})")
    print("=" * 50)

    prompt = "–ü–µ—Ä–µ—á–∏—Å–ª–∏ 5 –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤."

    print(f"\nüìù Prompt: {prompt}")
    print("\nüîÑ Streaming response:\n")

    start_time = time.time()
    first_token_time = None
    full_response = ""

    stream = ollama.chat(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        stream=True
    )

    for chunk in stream:
        if first_token_time is None:
            first_token_time = time.time() - start_time

        content = chunk["message"]["content"]
        full_response += content
        print(content, end="", flush=True)

    elapsed = time.time() - start_time

    print("\n\n" + "=" * 50)
    print("üìä Streaming Results:")
    print("=" * 50)
    print(f"‚è±Ô∏è  Time to first token: {first_token_time:.2f}s")
    print(f"‚è±Ô∏è  Total time: {elapsed:.2f}s")
    print(f"üìè Response length: {len(full_response)} chars")

    return full_response, first_token_time, elapsed


def test_llm():
    """Run all LLM tests"""
    # Check available models
    print("üìã Available models:")
    models = ollama.list()
    for m in models.models:
        print(f"   - {m.model}")

    # Use available model
    model = "qwen2.5:7b"

    # Run tests
    test_llm_basic(model)
    test_llm_streaming(model)


if __name__ == "__main__":
    test_llm()
